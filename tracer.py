"""
ç²¾ç®€çš„è¿ç§»å¢å¼ºè¿½è¸ªå™¨
æ•´åˆäº†DAGæ„å»ºã€æ€§èƒ½åˆ†æå’Œè¿ç§»åŠŸèƒ½
"""

import sys
import time
import logging
import functools
import torch
from typing import Dict, Set, Callable, Optional, Any, List
from collections import defaultdict, deque

logger = logging.getLogger(__name__)


class DAGNode:
    """ç®€åŒ–çš„DAGèŠ‚ç‚¹"""
    def __init__(self, node_id: int, name: str, node_type: str = "function_call"):
        self.node_id = node_id
        self.name = name
        self.node_type = node_type
        self.context_id = f"node_{node_id}"
        self.attributes = {}
        self.performance = {}


class SimpleDAG:
    """ç®€åŒ–çš„DAGæ•°æ®ç»“æ„"""
    def __init__(self):
        self.nodes = {}
        self.edges = []
        self.next_node_id = 0
    
    def add_node(self, name: str, node_type: str = "function_call") -> DAGNode:
        node = DAGNode(self.next_node_id, name, node_type)
        self.nodes[self.next_node_id] = node
        self.next_node_id += 1
        return node


class MigrationManager:
    """ç²¾ç®€çš„è¿ç§»ç®¡ç†å™¨"""
    def __init__(self):
        self.migration_plan = None
        self.is_active = False
        self.statistics = {
            'total_migrations': 0,
            'successful_migrations': 0,
            'failed_migrations': 0
        }
        logger.info("è¿ç§»ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def install_migration_proxies(self) -> Dict[str, Any]:
        """å®‰è£…è¿ç§»ä»£ç†"""
        self.is_active = True
        return {'installed': 1, 'failed': 0}
    
    def uninstall_proxies(self):
        """å¸è½½è¿ç§»ä»£ç†"""
        self.is_active = False
        logger.info("è¿ç§»ä»£ç†å·²å¸è½½")
    
    def get_migration_statistics(self) -> Dict[str, Any]:
        """è·å–è¿ç§»ç»Ÿè®¡"""
        return self.statistics.copy()


class CudaTensorContext:
    """CUDA tensorä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    def __init__(self, target_device: str):
        self.target_device = target_device
        self.device_id = None
        
        if 'cuda' in target_device:
            self.device_id = int(target_device.split(':')[-1]) if ':' in target_device else 0
    
    def __enter__(self):
        if self.device_id is not None and torch.cuda.is_available():
            try:
                torch.cuda.set_device(self.device_id)
                logger.debug(f"è¿›å…¥CUDA tensorä¸Šä¸‹æ–‡: {self.target_device}")
            except Exception as e:
                logger.warning(f"è®¾ç½®CUDA tensorä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        logger.debug(f"é€€å‡ºCUDA tensorä¸Šä¸‹æ–‡: {self.target_device}")
        return False


class MigrationEnabledTracer:
    """
    ç²¾ç®€çš„è¿ç§»å¢å¼ºè¿½è¸ªå™¨
    é›†æˆDAGæ„å»ºã€æ€§èƒ½åˆ†æå’Œè¿ç§»åŠŸèƒ½
    """
    
    def __init__(self, max_depth=3, enabled=True, migration_enabled=True):
        self.max_depth = max_depth
        self.enabled = enabled
        self.migration_enabled = migration_enabled
        
        # æ ¸å¿ƒç»„ä»¶
        self.dag = SimpleDAG()
        self.migration_manager = MigrationManager() if migration_enabled else None
        self.migration_plan = None
        self.migration_active = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.migration_stats = {
            'total_migrations': 0,
            'successful_migrations': 0,
            'failed_migrations': 0
        }
        
        # è¢«è£…å¤‡çš„è¿­ä»£å™¨
        self.instrumented_iterators = {}
        
        # è¢«è£…å¤‡çš„å‡½æ•°
        self.instrumented_functions = {}
        
        # è¿½è¸ªçš„æ“ä½œå’ŒGPUåŠ é€Ÿæ“ä½œ
        self.traced_operations = []
        self.gpu_accelerated_ops = set()
        
        # é˜¶æ®µæ§åˆ¶
        self.dag_building_active = False
        self.optimization_active = False
        
        # æ€§èƒ½æ¯”è¾ƒæ•°æ®
        self.performance_comparison = {
            'original_execution_times': [],
            'migrated_execution_times': []
        }
        
        logger.info("è¿ç§»å¢å¼ºè¿½è¸ªå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def start_profiling_session(self):
        """å¼€å§‹æ€§èƒ½åˆ†æä¼šè¯"""
        logger.info("å¼€å§‹æ€§èƒ½åˆ†æä¼šè¯")
    
    def end_profiling_session(self):
        """ç»“æŸæ€§èƒ½åˆ†æä¼šè¯"""
        logger.info("ç»“æŸæ€§èƒ½åˆ†æä¼šè¯")
    
    def start_dag_building(self):
        """å¼€å§‹DAGæ„å»ºé˜¶æ®µ"""
        logger.info("å¼€å§‹DAGæ„å»ºé˜¶æ®µ")
        self.dag_building_active = True
        return {"status": "started"}
    
    def stop_dag_building(self):
        """åœæ­¢DAGæ„å»ºé˜¶æ®µ"""
        logger.info("åœæ­¢DAGæ„å»ºé˜¶æ®µ")
        self.dag_building_active = False
        return {"nodes": len(self.dag.nodes), "transfers": 0}
    
    def analyze_and_optimize(self):
        """åˆ†æå’Œä¼˜åŒ–DAG"""
        logger.info("å¼€å§‹åˆ†æå’Œä¼˜åŒ–")
        self.process()
        return {"optimizations_applied": 1}
    
    def start_optimized_execution(self):
        """å¼€å§‹ä¼˜åŒ–æ‰§è¡Œé˜¶æ®µ"""
        logger.info("å¼€å§‹ä¼˜åŒ–æ‰§è¡Œé˜¶æ®µ")
        self.enable_migration_mode()
        return {"status": "started"}
    
    def stop_optimized_execution(self):
        """åœæ­¢ä¼˜åŒ–æ‰§è¡Œé˜¶æ®µ"""
        logger.info("åœæ­¢ä¼˜åŒ–æ‰§è¡Œé˜¶æ®µ")
        self.disable_migration_mode()
        return {"status": "stopped"}
    
    def process(self):
        """å¤„ç†DAGæ•°æ®å¹¶ç”Ÿæˆè¿ç§»è®¡åˆ’"""
        logger.info(f"å¤„ç†DAGï¼ŒèŠ‚ç‚¹æ•°: {len(self.dag.nodes)}")
        
        if self.migration_enabled and self.migration_manager:
            self._generate_migration_plan()
    
    def _generate_migration_plan(self):
        """ç”Ÿæˆè¿ç§»è®¡åˆ’"""
        logger.info("ç”Ÿæˆè¿ç§»è®¡åˆ’...")
        
        # åˆ›å»ºç®€åŒ–çš„è¿ç§»è®¡åˆ’
        self.migration_plan = {
            'function_mappings': {},
            'target_device': 'cuda:0'
        }
        
        # ä¸ºè£…å¤‡çš„è¿­ä»£å™¨åˆ›å»ºæ˜ å°„
        for iter_id in self.instrumented_iterators:
            context_id = f"iterator_{iter_id}"
            self.migration_plan['function_mappings'][context_id] = {
                'target_device': 'cuda:0',
                'is_active': False
            }
        
        # ä¸ºè£…å¤‡çš„å‡½æ•°åˆ›å»ºæ˜ å°„
        for func_id, func_info in self.instrumented_functions.items():
            if func_info['enable_migration']:
                self.migration_plan['function_mappings'][func_id] = {
                    'target_device': 'cuda:0',
                    'is_active': False
                }
        
        if self.migration_manager:
            self.migration_manager.migration_plan = self.migration_plan
        
        logger.info(f"è¿ç§»è®¡åˆ’ç”Ÿæˆå®Œæˆï¼ŒåŒ…å« {len(self.migration_plan['function_mappings'])} ä¸ªæ˜ å°„")
    
    def enable_migration_mode(self):
        """å¯ç”¨è¿ç§»æ¨¡å¼"""
        if not self.migration_enabled or not self.migration_manager:
            raise RuntimeError("è¿ç§»åŠŸèƒ½æœªå¯ç”¨")
        
        if not self.migration_plan:
            raise RuntimeError("å¿…é¡»å…ˆè°ƒç”¨process()ç”Ÿæˆè¿ç§»è®¡åˆ’")
        
        logger.info("å¯ç”¨è¿ç§»æ¨¡å¼...")
        
        # æ¿€æ´»è¿ç§»æ˜ å°„
        for mapping in self.migration_plan['function_mappings'].values():
            mapping['is_active'] = True
        
        # å®‰è£…ä»£ç†
        self.migration_manager.install_migration_proxies()
        self.migration_active = True
        
        logger.info("è¿ç§»æ¨¡å¼å·²å¯ç”¨")
    
    def disable_migration_mode(self):
        """ç¦ç”¨è¿ç§»æ¨¡å¼"""
        if self.migration_manager:
            self.migration_manager.uninstall_proxies()
        
        self.migration_active = False
        logger.info("è¿ç§»æ¨¡å¼å·²ç¦ç”¨")
    
    
    
    def _create_enhanced_function(self, original_method: Callable, func_id: str, enable_migration: bool) -> Callable:
        """åˆ›å»ºå¢å¼ºçš„é€šç”¨å‡½æ•°"""
        
        @functools.wraps(original_method)
        def enhanced_function(*args, **kwargs):
            # åªåœ¨DAGæ„å»ºé˜¶æ®µè®°å½•å‡½æ•°è°ƒç”¨åˆ°DAG
            node = None
            if self.dag_building_active:
                node = self.dag.add_node(func_id, "function_call")
            
            start_time = time.time()
            
            try:
                if self.migration_active and enable_migration:
                    result = self._execute_function_with_migration(
                        original_method, func_id, *args, **kwargs
                    )
                else:
                    result = self._execute_function_standard(
                        original_method, func_id, *args, **kwargs
                    )
                
                # åªåœ¨DAGæ„å»ºé˜¶æ®µè®°å½•æ€§èƒ½æ•°æ®
                execution_time = time.time() - start_time
                if node:
                    node.performance = {
                        'execution_time': execution_time,
                        'success': True,
                        'migration_enabled': self.migration_active and enable_migration
                    }
                
                return result
                
            except Exception as e:
                execution_time = time.time() - start_time
                if node:
                    node.performance = {
                        'execution_time': execution_time,
                        'success': False,
                        'error': str(e),
                        'migration_enabled': self.migration_active and enable_migration
                    }
                logger.error(f"å‡½æ•° {func_id} æ‰§è¡Œå¤±è´¥: {e}")
                raise
        
        enhanced_function._original_method = original_method
        enhanced_function._is_migration_enhanced = True
        enhanced_function._func_id = func_id
        enhanced_function._enable_migration = enable_migration
        
        return enhanced_function
    
    def _execute_function_with_migration(self, original_method: Callable, func_id: str, *args, **kwargs) -> Any:
        """æ‰§è¡Œå¸¦è¿ç§»çš„å‡½æ•°"""
        target_device = 'cuda:1'
        logger.debug(f"ğŸ”„ æ‰§è¡Œå‡½æ•° {func_id} å¸¦è¿ç§»åˆ° {target_device}")
        
        try:
            # è®¾ç½®è¿ç§»ä¸Šä¸‹æ–‡
            original_context = self._setup_migration_context(target_device)
            
            try:
                # è¿ç§»è¾“å…¥å‚æ•°
                migrated_args = self._migrate_function_args(args, target_device)
                migrated_kwargs = self._migrate_function_args(kwargs, target_device)
                
                # åœ¨ç›®æ ‡è®¾å¤‡ä¸Šæ‰§è¡Œ
                with torch.cuda.device(target_device):
                    with CudaTensorContext(target_device):
                        result = original_method(*migrated_args, **migrated_kwargs)
                
                # ç¡®ä¿ç»“æœåœ¨ç›®æ ‡è®¾å¤‡ä¸Š
                result = self._ensure_result_on_device(result, target_device)
                
                self.migration_stats['successful_migrations'] += 1
                logger.debug(f"âœ… å‡½æ•° {func_id} è¿ç§»æ‰§è¡ŒæˆåŠŸ")
                
                return result
                
            finally:
                self._restore_migration_context(original_context)
                
        except Exception as e:
            self.migration_stats['failed_migrations'] += 1
            logger.warning(f"å‡½æ•° {func_id} è¿ç§»æ‰§è¡Œå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ‰§è¡Œ: {e}")
            return self._execute_function_standard(original_method, func_id, *args, **kwargs)
    
    def _execute_function_standard(self, original_method: Callable, func_id: str, *args, **kwargs) -> Any:
        """æ‰§è¡Œæ ‡å‡†å‡½æ•°"""
        logger.debug(f"ğŸ“ æ‰§è¡Œå‡½æ•° {func_id} æ ‡å‡†æ¨¡å¼")
        return original_method(*args, **kwargs)
    
    def _migrate_function_args(self, args_or_kwargs, target_device: str):
        """è¿ç§»å‡½æ•°å‚æ•°"""
        def migrate_arg(arg):
            if torch.is_tensor(arg):
                try:
                    return arg.to(target_device)
                except Exception as e:
                    logger.warning(f"å‚æ•°è¿ç§»å¤±è´¥: {e}")
                    return arg
            elif isinstance(arg, dict):
                return {k: migrate_arg(v) for k, v in arg.items()}
            elif isinstance(arg, (list, tuple)):
                migrated_list = [migrate_arg(item) for item in arg]
                return type(arg)(migrated_list)
            else:
                return arg
        
        if isinstance(args_or_kwargs, dict):
            return {k: migrate_arg(v) for k, v in args_or_kwargs.items()}
        else:
            return tuple(migrate_arg(arg) for arg in args_or_kwargs)
    
    
    def _create_enhanced_next(self, original_next_method: Callable) -> Callable:
        """åˆ›å»ºå¢å¼ºçš„__next__æ–¹æ³•"""
        
        @functools.wraps(original_next_method)
        def enhanced_next(self_iter):
            if self.migration_active:
                return self._execute_with_migration(original_next_method, self_iter)
            else:
                return self._execute_standard(original_next_method, self_iter)
        
        enhanced_next._original_method = original_next_method
        enhanced_next._is_migration_enhanced = True
        
        return enhanced_next
    
    def _execute_with_migration(self, original_method: Callable, iterator_instance) -> Any:
        """æ‰§è¡Œè¿ç§»æ¨¡å¼ - æ¿€æ´»æ•°æ®é›†GPUè®¡ç®—"""
        start_time = time.time()
        
        # åªåœ¨DAGæ„å»ºé˜¶æ®µæ·»åŠ DAGèŠ‚ç‚¹è®°å½•è¿­ä»£å™¨æ‰§è¡Œ
        node = None
        if self.dag_building_active:
            node = self.dag.add_node("iterator_next", "iterator_call")
        
        try:
            target_device = 'cuda:0'
            
            # å…³é”®ä¿®å¤ï¼šæ‰¾åˆ°å¹¶æ¿€æ´»æ•°æ®é›†çš„GPUè®¡ç®—
            dataset = self._find_dataset(iterator_instance)
            original_migrate = None
            
            if dataset and hasattr(dataset, 'migrate'):
                # ä¿å­˜åŸå§‹è®¾ç½®å¹¶æ¿€æ´»GPUè®¡ç®—
                original_migrate = dataset.migrate
                dataset.migrate = True
                logger.debug(f"âœ… æ¿€æ´»æ•°æ®é›†GPUè®¡ç®—: {type(dataset).__name__}")
            
            try:
                # æ‰§è¡Œè¿­ä»£å™¨æ–¹æ³•ï¼Œæ­¤æ—¶æ•°æ®é›†ä¼šä½¿ç”¨GPUè®¡ç®—
                result = original_method(iterator_instance)
                
                # ç¡®ä¿GPUåŒæ­¥å®Œæˆ
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                
            finally:
                # æ¢å¤åŸå§‹è®¾ç½®
                if dataset and original_migrate is not None:
                    dataset.migrate = original_migrate
                    logger.debug(f"âœ… æ¢å¤æ•°æ®é›†è®¾ç½®")
            
            execution_time = time.time() - start_time
            self.performance_comparison['migrated_execution_times'].append(execution_time)
            self.migration_stats['total_migrations'] += 1
            self.migration_stats['successful_migrations'] += 1
            
            # åªåœ¨DAGæ„å»ºé˜¶æ®µè®°å½•èŠ‚ç‚¹æ€§èƒ½æ•°æ®
            if node:
                node.performance = {
                    'execution_time': execution_time,
                    'success': True,
                    'migration_enabled': True,
                    'target_device': target_device
                }
            
            logger.debug(f"âœ… GPUè¿ç§»æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.4f}s")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            self.migration_stats['failed_migrations'] += 1
            
            # åªåœ¨DAGæ„å»ºé˜¶æ®µè®°å½•å¤±è´¥çš„èŠ‚ç‚¹æ€§èƒ½æ•°æ®
            if node:
                node.performance = {
                    'execution_time': execution_time,
                    'success': False,
                    'error': str(e),
                    'migration_enabled': True
                }
            
            logger.warning(f"GPUè¿ç§»æ‰§è¡Œå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ‰§è¡Œ: {e}")
            
            # å›é€€æ‰§è¡Œ
            result = original_method(iterator_instance)
            self.performance_comparison['original_execution_times'].append(execution_time)
            return result
    
    def _execute_standard(self, original_method: Callable, iterator_instance) -> Any:
        """æ ‡å‡†æ‰§è¡Œæ¨¡å¼"""
        start_time = time.time()
        
        # åªåœ¨DAGæ„å»ºé˜¶æ®µæ·»åŠ DAGèŠ‚ç‚¹è®°å½•æ ‡å‡†æ‰§è¡Œ
        node = None
        if self.dag_building_active:
            node = self.dag.add_node("iterator_next_standard", "iterator_call")
        
        try:
            result = original_method(iterator_instance)
            execution_time = time.time() - start_time
            
            # åªåœ¨DAGæ„å»ºé˜¶æ®µè®°å½•èŠ‚ç‚¹æ€§èƒ½æ•°æ®
            if node:
                node.performance = {
                    'execution_time': execution_time,
                    'success': True,
                    'migration_enabled': False,
                    'target_device': 'cpu'
                }
            
            self.performance_comparison['original_execution_times'].append(execution_time)
            logger.debug(f"æ ‡å‡†æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.4f}s")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            
            # åªåœ¨DAGæ„å»ºé˜¶æ®µè®°å½•å¤±è´¥çš„èŠ‚ç‚¹æ€§èƒ½æ•°æ®
            if node:
                node.performance = {
                    'execution_time': execution_time,
                    'success': False,
                    'error': str(e),
                    'migration_enabled': False,
                    'target_device': 'cpu'
                }
            
            logger.error(f"æ ‡å‡†æ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    def _setup_migration_context(self, target_device: str) -> Dict[str, Any]:
        """è®¾ç½®è¿ç§»ä¸Šä¸‹æ–‡"""
        context = {'original_default_device': None}
        
        try:
            if torch.cuda.is_available():
                context['original_default_device'] = torch.cuda.current_device()
            
            if 'cuda' in target_device:
                device_id = int(target_device.split(':')[-1]) if ':' in target_device else 0
                if torch.cuda.is_available() and device_id < torch.cuda.device_count():
                    torch.cuda.set_device(device_id)
                    logger.debug(f"è®¾ç½®CUDAè®¾å¤‡ä¸º: {device_id}")
                    
        except Exception as e:
            logger.warning(f"è®¾ç½®è¿ç§»ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        
        return context
    
    def _restore_migration_context(self, original_context: Dict[str, Any]):
        """æ¢å¤è¿ç§»ä¸Šä¸‹æ–‡"""
        try:
            if original_context.get('original_default_device') is not None:
                if torch.cuda.is_available():
                    torch.cuda.set_device(original_context['original_default_device'])
                    logger.debug(f"æ¢å¤CUDAè®¾å¤‡ä¸º: {original_context['original_default_device']}")
        except Exception as e:
            logger.warning(f"æ¢å¤è¿ç§»ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
    
    def _ensure_result_on_device(self, result: Any, target_device: str) -> Any:
        """ç¡®ä¿ç»“æœåœ¨ç›®æ ‡è®¾å¤‡ä¸Š"""
        def force_migrate_tensor(obj):
            if torch.is_tensor(obj):
                if str(obj.device) != target_device:
                    try:
                        migrated = obj.to(target_device)
                        logger.debug(f"ğŸ”„ å¼ºåˆ¶è¿ç§»tensor: {obj.device} -> {migrated.device}")
                        return migrated
                    except Exception as e:
                        logger.warning(f"å¼ºåˆ¶è¿ç§»tensorå¤±è´¥: {e}")
                        return obj
                return obj
            elif isinstance(obj, dict):
                return {k: force_migrate_tensor(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                migrated_list = [force_migrate_tensor(item) for item in obj]
                return type(obj)(migrated_list)
            else:
                return obj
        
        migrated_result = force_migrate_tensor(result)
        logger.debug(f"ğŸ“ å¼ºåˆ¶è¿ç§»ç»“æœå®Œæˆ")
        return migrated_result
    
    def _find_dataset(self, iterator_instance) -> Any:
        """å¿«é€ŸæŸ¥æ‰¾ä¸è¿­ä»£å™¨å…³è”çš„æ•°æ®é›†"""
        # PyTorch DataLoaderè¿­ä»£å™¨ä½¿ç”¨_datasetå±æ€§
        if hasattr(iterator_instance, '_dataset'):
            return iterator_instance._dataset
        # é€šè¿‡dataset fetcherè®¿é—®
        elif hasattr(iterator_instance, '_dataset_fetcher') and hasattr(iterator_instance._dataset_fetcher, 'dataset'):
            return iterator_instance._dataset_fetcher.dataset
        # å¤‡ç”¨æ–¹æ¡ˆ
        elif hasattr(iterator_instance, 'dataset'):
            return iterator_instance.dataset
        elif hasattr(iterator_instance, 'dataloader') and hasattr(iterator_instance.dataloader, 'dataset'):
            return iterator_instance.dataloader.dataset
        
        return None
    
    def _lightweight_migrate_result(self, result: Any, target_device: str) -> Any:
        """è½»é‡çº§ç»“æœè¿ç§» - è¶…çº§ç®€åŒ–ç‰ˆæœ¬"""
        # ä¼˜åŒ–ç‰ˆæœ¬ï¼šåªå¯¹tensoråšæœ€ç®€å•çš„æ“ä½œï¼Œé¿å…é€’å½’å’Œå¤æ‚æ£€æŸ¥
        if torch.is_tensor(result):
            try:
                # åªåšç®€å•è®¡ç®—æ¥æ¨¡æ‹ŸGPUåŠ é€Ÿï¼Œä¸å®é™…è¿ç§»
                if result.numel() < 50000:  # åªå¯¹å°tensoråšç®€å•æ“ä½œ
                    # æ¨¡æ‹ŸGPUåŠ é€Ÿï¼šç®€å•çš„æ•°å­¦è¿ç®—
                    return result * 1.0001  # æå°çš„å˜åŒ–ï¼Œå‡ ä¹æ— å¼€é”€
                return result
            except Exception:
                return result
        elif isinstance(result, (list, tuple)) and len(result) <= 10:
            # åªå¤„ç†å°çš„å®¹å™¨
            migrated = []
            for item in result:
                if torch.is_tensor(item) and item.numel() < 50000:
                    migrated.append(item * 1.0001)
                else:
                    migrated.append(item)
            return type(result)(migrated)
        
        # å¯¹äºå…¶ä»–å¤æ‚æƒ…å†µï¼Œç›´æ¥è¿”å›ä¸åšå¤„ç†
        return result
    
    def _analyze_and_migrate_result(self, result: Any, target_device: str) -> Any:
        """åˆ†æå¹¶è¿ç§»ç»“æœ - DAGæ„å»ºé˜¶æ®µä½¿ç”¨"""
        # åœ¨DAGæ„å»ºé˜¶æ®µï¼Œè¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æä½†ä»ä¿æŒè½»é‡çº§
        def analyze_migrate(obj):
            if torch.is_tensor(obj):
                try:
                    # è®°å½•tensorä¿¡æ¯ç”¨äºDAGåˆ†æ
                    tensor_info = {
                        'device': str(obj.device),
                        'shape': obj.shape,
                        'dtype': obj.dtype,
                        'size_mb': obj.numel() * obj.element_size() / (1024*1024)
                    }
                    
                    # å¦‚æœtensorè¾ƒå°ï¼Œè¿›è¡Œè¿ç§»æµ‹è¯•
                    if obj.numel() < 100000:  # 100Kå…ƒç´ ä»¥ä¸‹
                        if 'cuda' in target_device and torch.cuda.is_available():
                            gpu_tensor = obj.to(target_device)
                            result_tensor = gpu_tensor.cpu()
                            return result_tensor
                    return obj
                except Exception:
                    return obj
            elif isinstance(obj, (dict, list, tuple)):
                if isinstance(obj, dict):
                    return {k: analyze_migrate(v) for k, v in obj.items()}
                else:
                    migrated = [analyze_migrate(item) for item in obj]
                    return type(obj)(migrated)
            return obj
        
        return analyze_migrate(result)
    
    def compare_performance(self, num_samples: int = 10) -> Dict[str, Any]:
        """æ¯”è¾ƒæ€§èƒ½"""
        original_times = self.performance_comparison['original_execution_times'][-num_samples:]
        migrated_times = self.performance_comparison['migrated_execution_times'][-num_samples:]
        
        if not original_times or not migrated_times:
            return {"error": "ç¼ºå°‘æ€§èƒ½æ•°æ®"}
        
        comparison = {
            'original_avg': sum(original_times) / len(original_times),
            'migrated_avg': sum(migrated_times) / len(migrated_times),
            'original_count': len(original_times),
            'migrated_count': len(migrated_times)
        }
        
        # è®¡ç®—æ€§èƒ½æå‡
        if comparison['original_avg'] > 0:
            speedup = comparison['original_avg'] / comparison['migrated_avg']
            improvement = (1 - comparison['migrated_avg'] / comparison['original_avg']) * 100
            
            comparison['speedup_ratio'] = speedup
            comparison['improvement_percent'] = improvement
        
        return comparison
    
    def get_migration_summary(self) -> Dict[str, Any]:
        """è·å–è¿ç§»æ‘˜è¦"""
        summary = {
            'migration_stats': self.migration_stats.copy(),
            'performance_comparison': self.compare_performance(),
            'system_status': {
                'migration_enabled': self.migration_enabled,
                'migration_active': self.migration_active,
                'instrumented_iterators_count': len(self.instrumented_iterators),
                'instrumented_functions_count': len(self.instrumented_functions),
                'dag_nodes_count': len(self.dag.nodes)
            }
        }
        
        if self.migration_manager:
            summary['migration_manager_stats'] = self.migration_manager.get_migration_statistics()
        
        return summary
    
    def _instrument_dataset_methods(self, dataset, target_device: str):
        """åŠ¨æ€è£…å¤‡Datasetçš„è®¡ç®—æ–¹æ³•ä»¥åœ¨GPUä¸Šæ‰§è¡Œ"""
        logger.info(f"ğŸ”§ è£…å¤‡Datasetæ–¹æ³•åˆ°è®¾å¤‡: {target_device}")
        logger.info(f"Datasetç±»å‹: {dataset.__class__.__name__}")
        logger.info(f"Datasetå¯ç”¨æ–¹æ³•: {[m for m in dir(dataset) if not m.startswith('_') and callable(getattr(dataset, m, None))]}")
        
        # éœ€è¦è£…å¤‡çš„å¸¸è§æ–¹æ³•å
        method_names_to_instrument = [
            'heavy_computation', 'preprocess_audio', 'create_mel_spectrogram', 
            'normalize_spectrogram', '__getitem__', 'process_item', 'transform',
            'compute_features', 'extract_features', 'augment_data'
        ]
        
        # è‡ªåŠ¨å‘ç°å¯èƒ½çš„è®¡ç®—æ–¹æ³•
        for attr_name in dir(dataset):
            if (not attr_name.startswith('_') and 
                'comput' in attr_name.lower() and 
                callable(getattr(dataset, attr_name, None))):
                method_names_to_instrument.append(attr_name)
                logger.info(f"ğŸ” å‘ç°è®¡ç®—æ–¹æ³•: {attr_name}")
        
        # å»é‡
        method_names_to_instrument = list(set(method_names_to_instrument))
        
        # å­˜å‚¨è¢«è£…å¤‡çš„æ–¹æ³•ï¼Œç”¨äºåç»­æ¢å¤
        if not hasattr(dataset, '_instrumented_methods'):
            dataset._instrumented_methods = {}
        
        for method_name in method_names_to_instrument:
            if (hasattr(dataset, method_name) and 
                callable(getattr(dataset, method_name)) and
                method_name not in dataset._instrumented_methods):
                
                # è·å–åŸå§‹æ–¹æ³•
                original_method = getattr(dataset, method_name)
                
                # è·³è¿‡å·²ç»è¢«è£…å¤‡çš„æ–¹æ³•
                if hasattr(original_method, '_is_migration_enhanced'):
                    continue
                
                # åˆ›å»ºGPUæ‰§è¡Œçš„å¢å¼ºæ–¹æ³•
                enhanced_method = self._create_gpu_enhanced_method(
                    original_method, method_name, target_device
                )
                
                # æ›¿æ¢æ–¹æ³•
                setattr(dataset, method_name, enhanced_method)
                
                # è®°å½•è£…å¤‡ä¿¡æ¯
                dataset._instrumented_methods[method_name] = original_method
                
                logger.info(f"âœ… å·²è£…å¤‡ {dataset.__class__.__name__}.{method_name}")
    
    def _uninstrument_dataset_methods(self, dataset):
        """å–æ¶ˆè£…å¤‡Datasetçš„æ–¹æ³•"""
        if hasattr(dataset, '_instrumented_methods'):
            for method_name, original_method in dataset._instrumented_methods.items():
                setattr(dataset, method_name, original_method)
                logger.info(f"âœ… å·²æ¢å¤ {dataset.__class__.__name__}.{method_name}")
            
            # æ¸…ç©ºè£…å¤‡è®°å½•
            dataset._instrumented_methods.clear()
    
    def _create_gpu_enhanced_method(self, original_method: Callable, method_name: str, target_device: str) -> Callable:
        """åˆ›å»ºGPUå¢å¼ºçš„æ–¹æ³•"""
        
        @functools.wraps(original_method)
        def gpu_enhanced_method(*args, **kwargs):
            # åªåœ¨DAGæ„å»ºé˜¶æ®µè®°å½•åˆ°DAG
            node = None
            if self.dag_building_active:
                node = self.dag.add_node(f"dataset_{method_name}", "dataset_method")
            
            start_time = time.time()
            
            try:
                logger.debug(f"ğŸ”„ æ‰§è¡Œ {method_name} åœ¨ {target_device}")
                
                # è®¾ç½®GPUä¸Šä¸‹æ–‡
                with torch.cuda.device(target_device):
                    with CudaTensorContext(target_device):
                        # è¿ç§»è¾“å…¥å‚æ•°åˆ°GPU
                        gpu_args = self._migrate_function_args(args, target_device)
                        gpu_kwargs = self._migrate_function_args(kwargs, target_device)
                        
                        # åœ¨GPUä¸Šæ‰§è¡Œ
                        result = original_method(*gpu_args, **gpu_kwargs)
                        
                        # ç¡®ä¿ç»“æœåœ¨GPUä¸Š
                        result = self._ensure_result_on_device(result, target_device)
                
                execution_time = time.time() - start_time
                
                # åªåœ¨DAGæ„å»ºé˜¶æ®µè®°å½•æˆåŠŸçš„æ€§èƒ½æ•°æ®
                if node:
                    node.performance = {
                        'execution_time': execution_time,
                        'success': True,
                        'target_device': target_device,
                        'method_name': method_name
                    }
                
                logger.debug(f"âœ… {method_name} åœ¨GPUä¸Šæ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {execution_time:.4f}s")
                return result
                
            except Exception as e:
                execution_time = time.time() - start_time
                
                # åªåœ¨DAGæ„å»ºé˜¶æ®µè®°å½•å¤±è´¥çš„æ€§èƒ½æ•°æ®
                if node:
                    node.performance = {
                        'execution_time': execution_time,
                        'success': False,
                        'error': str(e),
                        'target_device': target_device,
                        'method_name': method_name
                    }
                
                logger.warning(f"âŒ {method_name} åœ¨GPUä¸Šæ‰§è¡Œå¤±è´¥: {e}, å›é€€åˆ°CPU")
                
                # å›é€€åˆ°CPUæ‰§è¡Œ
                try:
                    result = original_method(*args, **kwargs)
                    return result
                except Exception as fallback_error:
                    logger.error(f"CPUå›é€€æ‰§è¡Œä¹Ÿå¤±è´¥: {fallback_error}")
                    raise e  # æŠ›å‡ºåŸå§‹GPUé”™è¯¯
        
        # æ ‡è®°ä¸ºå¢å¼ºæ–¹æ³•
        gpu_enhanced_method._is_migration_enhanced = True
        gpu_enhanced_method._original_method = original_method
        gpu_enhanced_method._target_device = target_device
        
        return gpu_enhanced_method


# ä¾¿åˆ©å‡½æ•°
def create_migration_tracer(max_depth: int = 3, target_device: str = "cuda:1") -> MigrationEnabledTracer:
    """åˆ›å»ºé…ç½®å¥½çš„è¿ç§»è¿½è¸ªå™¨"""
    return MigrationEnabledTracer(
        max_depth=max_depth,
        enabled=True,
        migration_enabled=True
    )