
# æ•°æ®ç®¡é“è¿ç§»è®¡ç®—æ¨¡å—è®¾è®¡æ–‡æ¡£

## 1. æ¨¡å—å®šä½ä¸è¾¹ç•Œ
*   **è¾“å…¥**ï¼š
    1.  **ä¼˜åŒ–æ–¹æ¡ˆ (Optimization Plan)**ï¼šç”±å‰åºä¼˜åŒ–æ¨¡å—è¾“å‡ºï¼Œæè¿°äº†â€œå“ªäº›ä¸Šä¸‹æ–‡è·¯å¾„ï¼ˆContext Keyï¼‰ä¸‹çš„èŠ‚ç‚¹éœ€è¦è¿ç§»åˆ°å“ªä¸ªè®¾å¤‡â€ã€‚
        *   *Example*: `{"Dataset/__getitem__/preprocess": "cuda:0", "cv2.resize": "cuda:0"}`
    2.  **åŸå§‹æ•°æ®ç®¡é“ä»£ç **ï¼šç”¨æˆ·çš„ Python è„šæœ¬/é¡¹ç›®ã€‚
*   **è¾“å‡º**ï¼š
    *   **Patch åçš„è¿è¡Œæ—¶ç¯å¢ƒ**ï¼šå…³é”®å‡½æ•°è¢« Hookï¼Œå…·å¤‡è‡ªåŠ¨è¿ç§»èƒ½åŠ›ã€‚
*   **èŒè´£**ï¼šåœ¨ä¸ä¿®æ”¹ç”¨æˆ·æºç çš„å‰æä¸‹ï¼ŒåŸºäºä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŠ¨æ€åœ°å°† CPU è®¡ç®—è´Ÿè½½è°ƒåº¦è‡³ GPUã€‚

---

## 2. ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TD
    subgraph Configuration
        Plan[ä¼˜åŒ–æ–¹æ¡ˆ (Optimization Plan)]
    end

    subgraph Migration_Module [è¿ç§»è®¡ç®—æ¨¡å—]
        Registry[è¿ç§»ç­–ç•¥åº“ (Knowledge Base)]
        ContextMgr[ç¨€ç–ä¸Šä¸‹æ–‡è¿½è¸ªå™¨ (Context Tracker)]
        
        subgraph Patch_Engine [æ³¨å…¥ä¸æ‰§è¡Œå¼•æ“]
            Injector[Patch æ³¨å…¥å™¨]
            Wrapper[é€šç”¨åŒ…è£…å™¨ (Universal Wrapper)]
        end
    end

    subgraph Runtime
        UserCode[ç”¨æˆ·ä»£ç  / ç¬¬ä¸‰æ–¹åº“]
        GPU_Backend[GPU å®ç° (Kornia/Torch/etc.)]
    end

    Plan --> Injector
    Injector -->|1. æ›¿æ¢ç›®æ ‡å‡½æ•°| UserCode
    
    UserCode -->|2. è°ƒç”¨è¢«æ‹¦æˆªå‡½æ•°| Wrapper
    Wrapper -->|3. æ›´æ–°è·¯å¾„| ContextMgr
    Wrapper -->|4. æŸ¥è¯¢ç­–ç•¥| Registry
    Wrapper -->|5. æ‰§è¡Œè®¡ç®—| GPU_Backend
    
    Registry -.->|æä¾›è½¬æ¢é€»è¾‘| Wrapper
    ContextMgr -.->|æä¾›å½“å‰Key| Wrapper
```

---

## 3. å­æ¨¡å—è¯¦ç»†è®¾è®¡

### 3.1 è¿ç§»ç­–ç•¥åº“ (Migration Strategy Registry)
**èŒè´£**ï¼šå­˜å‚¨â€œå¦‚ä½•è¿ç§»â€çš„é™æ€çŸ¥è¯†ã€‚å®ƒæ˜¯æˆ‘ä»¬åœ¨ä¸Šä¸€è½®å¯¹è¯ä¸­æ¢³ç†çš„æ ¸å¿ƒæˆæœã€‚

*   **æ•°æ®ç»“æ„**ï¼š
    *   ç»´æŠ¤ä¸€ä¸ªå…¨å±€å­—å…¸ `Map<FunctionPath, MigrationStrategy>`ã€‚
*   **æ ¸å¿ƒå®ä½“ `MigrationStrategy`**ï¼š
    *   `input_processors`: è¾“å…¥å‚æ•°è½¬æ¢é“¾ï¼ˆå¦‚ `[EnsureTensor('cuda'), PassThrough] `ï¼‰ã€‚
    *   `arg_mapper`: å‚æ•°ç­¾åä¿®æ­£é€»è¾‘ï¼ˆå¦‚ `SwapArgs(0, 1)` ç”¨äº cv2.resizeï¼‰ã€‚
    *   `backend`: ç›®æ ‡ GPU å‡½æ•°ï¼ˆå¦‚ `kornia.geometry.resize` æˆ– `OriginalFunc`ï¼‰ã€‚
    *   `output_processor`: ç»“æœå¤„ç†ï¼ˆå¦‚ `KeepOnDevice` æˆ– `ToNumpy`ï¼‰ã€‚
*   **æ‰©å±•æ€§**ï¼šæ”¯æŒé€šè¿‡è£…é¥°å™¨æˆ–é…ç½®æ–‡ä»¶åŠ¨æ€æ³¨å†Œæ–°çš„åº“æ”¯æŒï¼ˆCV, Audio, NLPï¼‰ã€‚

### 3.2 ç¨€ç–ä¸Šä¸‹æ–‡è¿½è¸ªå™¨ (Sparse Context Tracker)
**èŒè´£**ï¼šç»´æŠ¤è¿è¡Œæ—¶è°ƒç”¨æ ˆï¼Œç”Ÿæˆ Context Key ä»¥åŒ¹é…ä¼˜åŒ–æ–¹æ¡ˆã€‚

*   **è®¾è®¡è¦ç‚¹**ï¼š
    *   **çº¿ç¨‹å®‰å…¨**ï¼šå¿…é¡»ä½¿ç”¨ `threading.local()`ï¼Œå› ä¸º `DataLoader` å¯èƒ½åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹å·¥ä½œï¼ˆè™½ç„¶ Python GIL é™åˆ¶äº†è®¡ç®—å¹¶è¡Œï¼Œä½† Context å¿…é¡»éš”ç¦»ï¼‰ã€‚
    *   **ç¨€ç–æ€§**ï¼šåªè®°å½•**è¢« Patch çš„å‡½æ•°**ã€‚
        *   çœŸå®è°ƒç”¨ï¼š`__getitem__` -> `func_A` -> `func_B` (æœªPatch) -> `cv2.resize` (Patch)ã€‚
        *   æ ˆçŠ¶æ€ï¼š`['__getitem__', 'func_A', 'cv2.resize']`ã€‚
        *   Keyï¼š`"__getitem__/func_A/cv2.resize"`ã€‚
*   **æ¥å£**ï¼š
    *   `enter(name)`: å…¥æ ˆã€‚
    *   `exit()`: å‡ºæ ˆã€‚
    *   `current_key()`: è·å–å½“å‰è·¯å¾„å­—ç¬¦ä¸²ã€‚

### 3.3 Patch æ³¨å…¥å™¨ (Patch Injector)
**èŒè´£**ï¼šåœ¨ç¨‹åºå¯åŠ¨é˜¶æ®µï¼Œæ ¹æ®ä¼˜åŒ–æ–¹æ¡ˆå®æ–½ Monkey Patchã€‚

*   **å·¥ä½œæµç¨‹**ï¼š
    1.  **è§£æè®¡åˆ’**ï¼šéå† `Optimization Plan` ä¸­çš„æ‰€æœ‰ Keyã€‚
    2.  **æå–ç›®æ ‡**ï¼šä» Key ä¸­æå–æœ«ç«¯å‡½æ•°åï¼ˆä¾‹å¦‚ä» `.../cv2.resize` æå– `cv2` æ¨¡å—å’Œ `resize` å‡½æ•°ï¼‰ã€‚
    3.  **å¤‡ä»½åŸå‡½æ•°**ï¼šå°† `original_func` ä¿å­˜åˆ° `Wrapper` çš„é—­åŒ…æˆ–å±æ€§ä¸­ï¼Œé˜²æ­¢æ— é™é€’å½’ã€‚
    4.  **å®æ–½æ›¿æ¢**ï¼š`setattr(module, func_name, UniversalWrapper(original_func, ...))`ã€‚
    5.  **ç‰¹æ®Šå¤„ç†**ï¼šé’ˆå¯¹ç±»æ–¹æ³•ï¼ˆå¦‚ `transforms.Resize`ï¼‰ï¼Œéœ€è¦ Patch ç±»çš„ `__call__` æˆ– `forward`ã€‚

### 3.4 é€šç”¨åŒ…è£…å™¨ (Universal Wrapper) â€”â€” **æ ¸å¿ƒæ‰§è¡Œå•å…ƒ**
**èŒè´£**ï¼šè¿è¡Œæ—¶çš„è°ƒåº¦å‘˜ã€‚è¿™æ˜¯æœ€å¤æ‚çš„ç»„ä»¶ï¼Œæ‰¿è½½äº†æ‰€æœ‰çš„æ§åˆ¶æµé€»è¾‘ã€‚

#### é€»è¾‘æµç¨‹ï¼ˆä¼ªä»£ç ï¼‰ï¼š

```python
def universal_wrapper(original_func, func_name):
    def wrapper(*args, **kwargs):
        # 1. ä¸Šä¸‹æ–‡å…¥æ ˆ
        context_tracker.enter(func_name)
        current_key = context_tracker.current_key()
        
        # 2. å†³ç­–ï¼šå½“å‰èŠ‚ç‚¹æ˜¯å¦åœ¨ä¼˜åŒ–è®¡åˆ’ä¸­ï¼Ÿä¸”ç›®æ ‡è®¾å¤‡æ˜¯ GPUï¼Ÿ
        target_device = optimization_plan.get(current_key)
        should_migrate = target_device and target_device.startswith('cuda')
        
        result = None
        try:
            if should_migrate:
                # --- GPU åˆ†æ”¯ ---
                
                # A. è·å–è¿ç§»ç­–ç•¥ (ä» Registry)
                # å¦‚æœæ˜¯ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼Œé€šå¸¸æ²¡æœ‰æ³¨å†Œç­–ç•¥ï¼Œåˆ™ä½¿ç”¨é»˜è®¤ç­–ç•¥ï¼ˆMove Input Onlyï¼‰
                strategy = registry.get(func_name, default=MoveInputStrategy)
                
                # B. å‚æ•°é¢„å¤„ç† (Input Processor)
                # å°† args/kwargs è½¬æ¢ä¸º Tensor å¹¶æ¬è¿åˆ° target_device
                gpu_args, gpu_kwargs = strategy.process_inputs(args, kwargs, target_device)
                
                # C. å‚æ•°æ˜ å°„ (Arg Mapper - å¤„ç†ç­¾åå·®å¼‚)
                if strategy.arg_mapper:
                    gpu_args, gpu_kwargs = strategy.arg_mapper(gpu_args, gpu_kwargs)
                
                # D. æ‰§è¡Œåç«¯ (Backend Execution)
                # å¯èƒ½æ˜¯ Kornia å‡½æ•°ï¼Œä¹Ÿå¯èƒ½æ˜¯åŸå‡½æ•°(ä¾èµ– PyTorch Dispatch)
                result = strategy.backend(*gpu_args, **gpu_kwargs)
                
                # E. ç»“æœåå¤„ç† (Output Processor)
                result = strategy.process_output(result)
                
            else:
                # --- CPU åˆ†æ”¯ (æœªå‘½ä¸­ä¼˜åŒ–è®¡åˆ’) ---
                result = original_func(*args, **kwargs)

        except Exception as e:
            # --- å®¹é”™é™çº§ (Fallback) ---
            logger.warning(f"Migration failed at {current_key}: {e}. Falling back to CPU.")
            # å¿…é¡»ç¡®ä¿è¾“å…¥æ•°æ®åœ¨ CPU (å¦‚æœä¹‹å‰è¢«éƒ¨åˆ†æ¬è¿äº†ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦å›é€€é€»è¾‘ï¼Œ
            # ä½†æœ€ç®€å•çš„æ˜¯ç›´æ¥ç”¨åŸå§‹ args è°ƒåŸå‡½æ•°)
            result = original_func(*args, **kwargs)
            
        finally:
            # 3. ä¸Šä¸‹æ–‡å‡ºæ ˆ
            context_tracker.exit()
            
        # 4. IPC è¾¹ç•Œæ£€æŸ¥ (IPC Guard) - é’ˆå¯¹å¤šè¿›ç¨‹ DataLoader
        if is_worker_process() and is_top_level_node(current_key):
             result = ensure_cpu(result)
             
        return result
    return wrapper
```

---

## 4. å…³é”®ç‰¹æ€§çš„å®ç°ä¿éšœ

### 4.1 æ··åˆç²’åº¦æ”¯æŒ
*   **ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•° (Function-Level)**ï¼š
    *   Registry ä¸­æ— è®°å½•ã€‚
    *   Wrapper ä½¿ç”¨é»˜è®¤ç­–ç•¥ï¼š`InputProcessors=[EnsureTensor(device)]`, `Backend=OriginalFunc`ã€‚
    *   **æ•ˆæœ**ï¼šä»…æ¬è¿ Tensorï¼Œå‡½æ•°å†…éƒ¨çš„ PyTorch ç®—å­è‡ªåŠ¨åœ¨ GPU æ‰§è¡Œã€‚
*   **ç¬¬ä¸‰æ–¹åº“å‡½æ•° (Operator-Level)**ï¼š
    *   Registry ä¸­æœ‰è®°å½•ï¼ˆå¦‚ `cv2.resize`ï¼‰ã€‚
    *   Wrapper ä½¿ç”¨æ³¨å†Œç­–ç•¥ï¼š`InputProcessors=[ImageHWC2CHW]`, `Backend=kornia.resize`ã€‚
    *   **æ•ˆæœ**ï¼šå®Œå…¨æ›¿æ¢å®ç°ã€‚

### 4.2 æ•°æ®é©»ç•™ (Data Residency / Lazy Transfer)
*   åœ¨ `InputProcessor` ä¸­å®ç°ã€‚
*   é€»è¾‘ï¼š`if isinstance(arg, Tensor) and arg.device == target_device: return arg`ã€‚
*   **æ”¶ç›Š**ï¼šå¦‚æœç®¡é“æ˜¯ `Resize(GPU) -> Rotate(GPU) -> Normalize(GPU)`ï¼Œåªæœ‰ç¬¬ä¸€ä¸ª `Resize` ä¼šè§¦å‘ CPU->GPU æ‹·è´ï¼Œåç»­æ“ä½œç›´æ¥å¤ç”¨ GPU æ•°æ®ï¼Œæ¶ˆé™¤ PCIe ç“¶é¢ˆã€‚

### 4.3 å¤šè¿›ç¨‹ IPC å®‰å…¨ (IPC Guard)
*   **é—®é¢˜**ï¼šPyTorch `DataLoader` çš„ `num_workers > 0` æ—¶ï¼Œå­è¿›ç¨‹è¿”å› GPU Tensor å¯èƒ½å¯¼è‡´ CUDA åˆå§‹åŒ–é”™è¯¯æˆ– IPC å¤±è´¥ï¼ˆé™¤éä½¿ç”¨ Shared Memoryï¼Œä½†å¤æ‚ï¼‰ã€‚
*   **æœºåˆ¶**ï¼šåœ¨ Wrapper çš„æœ€åï¼ˆReturn ä¹‹å‰ï¼‰ï¼Œæ£€æŸ¥ï¼š
    1.  å½“å‰æ˜¯å¦åœ¨ Worker è¿›ç¨‹ä¸­ï¼Ÿ
    2.  å½“å‰æ˜¯å¦æ˜¯ Context æ ˆçš„**æ ˆåº•**ï¼ˆå³æœ€å¤–å±‚è¢« Hook çš„å‡½æ•°ï¼Œé€šå¸¸æ˜¯ `__getitem__`ï¼‰ï¼Ÿ
    3.  å¦‚æœæ˜¯ï¼Œå¼ºåˆ¶æ‰§è¡Œ `.cpu()`ã€‚
*   **å¦¥å**ï¼šè¿™ç¡®å®å¼•å…¥äº† D->H çš„æ‹·è´ï¼Œä½†ä¿è¯äº†ç¨³å®šæ€§ã€‚å¯¹äºæè‡´æ€§èƒ½ï¼Œå»ºè®®ç”¨æˆ·è®¾ç½® `num_workers=0` å¹¶å®Œå…¨ä¾èµ– GPU çš„ååèƒ½åŠ›ã€‚

---

## 5. æ¨¡å—é›†æˆæ¥å£

è¯¥æ¨¡å—å¯¹å¤–æš´éœ²ç®€æ´çš„ APIï¼š

```python
class PipelineMigrator:
    def __init__(self, optimization_json_path: str):
        self.plan = load_plan(optimization_json_path)
        self.registry = default_registry() # åŠ è½½æˆ‘ä»¬æ¢³ç†å¥½çš„å››å¤§ç±»åº“
        self.tracker = SparseContextTracker()
        
    def activate(self):
        """å¯ç”¨è¿ç§»ï¼šæ‰§è¡Œ Monkey Patch"""
        injector = PatchInjector(self.plan, self.registry, self.tracker)
        injector.apply_all()
        print("ğŸ”¥ DPMCM Activated: GPU Migration Hooks Installed.")

    def deactivate(self):
        """æ¢å¤åŸå§‹ç¯å¢ƒ"""
        # æ¢å¤ _original_func
        pass
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
# ç”¨æˆ·ä»£ç 
migrator = PipelineMigrator("plan.json")
migrator.activate()

# ... æ­£å¸¸è¿è¡Œæ•°æ®åŠ è½½ ...
for data in dataloader:
    pass 
```

è¿™ä¸ªè®¾è®¡æ–‡æ¡£æ¶µç›–äº†ä»ç­–ç•¥å®šä¹‰åˆ°è¿è¡Œæ—¶æ‰§è¡Œçš„å®Œæ•´é“¾è·¯ï¼Œæ—¢ä¿è¯äº†çµæ´»æ€§ï¼ˆRegistryï¼‰ï¼Œåˆä¿è¯äº†ç¨³å®šæ€§ï¼ˆIPC Guard/Fallbackï¼‰ã€‚